{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import shutil\n",
    "import pymysql\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run the following mongodb query to unwind the collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db.CX_commissions_etl.drop()\n",
    "\n",
    "# db.CXcommissions_201808201535.aggregate([\n",
    "# {$unwind: {path: \"$TOURCODES\", preserveNullAndEmptyArrays: true}},\n",
    "# {$unwind: {path: \"$ORIGINATING\", preserveNullAndEmptyArrays: true}},\n",
    "# {$unwind: {path: \"$FLIGHTS\", preserveNullAndEmptyArrays: true}},\n",
    "# {$unwind: {path: \"$POS\", preserveNullAndEmptyArrays: true}},\n",
    "# {$unwind: {path: \"$FROM_LOCATION\", preserveNullAndEmptyArrays: true}},\n",
    "# {$unwind: {path: \"$TO_LOCATION\", preserveNullAndEmptyArrays: true}},\n",
    "# {$unwind: {path: \"$PCC\", preserveNullAndEmptyArrays: true}},\n",
    "# {$unwind: {path: \"$FBC\", preserveNullAndEmptyArrays: true}},\n",
    "# {$project : {_id : 0 }},\n",
    "# {$out:\"CXcommissions_etl\"}\n",
    "# ]);\n",
    "\n",
    "# db.CXcommissions_etl.find({}).count()\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client=MongoClient('local')\n",
    "db = client.raxdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [doc for doc in db[\"CXcommissions_etl\"].find({})]\n",
    "data = pd.DataFrame(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change columns to lower case\n",
    "data.columns = [col.lower() for col in data.columns]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the pos column as the country column, drop the country and route column\n",
    "data['pos']=data['country']\n",
    "data.drop('country', inplace= True, axis=1)\n",
    "# drop the route \n",
    "data.drop('route', axis =1, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace nan/null with empty string\n",
    "data=data.fillna(\"\")\n",
    "#data.loc[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update unknown date to '9999-12-31' \n",
    "date_cols = ['sales_period_dis_date', 'sales_period_eff_date', 'tkt_dis_date', 'trv_dis_date']\n",
    "for date_col in date_cols:\n",
    "    update_mask = data[date_col]==\"\"\n",
    "    data.loc[update_mask, date_col]='9999-12-31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore\n",
    "# update float, ignore row if it can't\n",
    "# float_cols = ['agent_discount', 'corporate_discount']\n",
    "# for index, row in data.iterrows():\n",
    "#     for float_col in float_cols:\n",
    "#         float_num = row[float_col]\n",
    "#         data.loc[index, float_col] = float(format(float_num, '.6f'))\n",
    "#         print(\"Updateing: \", index, float_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore the following step:\n",
    "# get rid of '+' and '-' in columns\n",
    "# cols = ['originating']\n",
    "# for index, row in data.iterrows():\n",
    "#     for col in cols:\n",
    "#         string = row[col]\n",
    "#         if string:\n",
    "#             string = str(string).replace('-', '').replace('+', '')\n",
    "#             data.loc[index, col] = string\n",
    "#             print(\"Updateing: \", index, string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update 9999 to None\n",
    "cols = ['ct_fare_comm', 'rtw_fare_comm']\n",
    "for col in cols:\n",
    "    update_mask = data[col]==9999\n",
    "    data.loc[update_mask, col]= \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace \"'BOS'\" with \"BOS\"\n",
    "data['from_location']=data['from_location'].apply(lambda x: x.replace(\"'\", \"\"))\n",
    "data['fbc']=data['fbc'].apply(lambda x: x.replace(\"'\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check values of a column\n",
    "data['from_location'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location 'OR' is in the location information, need to clean it\n",
    "mask = data['from_location']=='OR'\n",
    "data.drop(data.loc[mask].index, inplace=True)\n",
    "mask = data['to_location']=='OR'\n",
    "data.drop(data.loc[mask].index, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unique values of columns\n",
    "def check_unique_values(cols=None):\n",
    "    if cols:\n",
    "        cols=cols\n",
    "    else:\n",
    "        cols = data.columns\n",
    "    for col in cols:\n",
    "        print(\"Columns: \", col)      \n",
    "        print(data[col].unique())\n",
    "check_unique_values(['agent_discount', 'corporate_acc_id', 'corporate_discount',\n",
    "       'ct_fare_comm', 'discount_unit', 'fbc', 'flights', 'pcc', 'pos', 'rbd', 'rtw_fare_comm',\n",
    "       'sales_period_dis_date', 'sales_period_eff_date', 'tkt_designator',\n",
    "       'tkt_dis_date', 'trv_dis_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_values(data, col, old_val, update_val):\n",
    "    mask = data[col]==old_val\n",
    "    data.loc[mask, col] = update_val\n",
    "# delete row if a column contains certain value\n",
    "def delete_values(data, col, val):\n",
    "    mask = data[col]==val\n",
    "    data.drop(data.loc[mask].index, inplace=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning:\n",
    "# case1: Columns:  agent_discount\n",
    "update_values(data, \"agent_discount\", \"10\", 10.0)\n",
    "# case2:\n",
    "re_val=[ '3%\\nALL TICKETS MUST BE ISSUED\\r\\n  ON/BEFORE 31DEC2016',\n",
    " '3% \\nALL TICKETS MUST BE ISSUED\\r\\n  ON/BEFORE 14AUG2016',\n",
    " '3%\\nALL TICKETS MUST BE ISSUED\\r\\n  ON/BEFORE 14AUG2016']\n",
    "for v in re_val:\n",
    "    update_values(data, \"agent_discount\", v, 3.0)   \n",
    "# case 3: \n",
    "update_values(data, \"agent_discount\", 'W W-TYPE 10%I II I' , 10.0)\n",
    "update_values(data, \"agent_discount\", 'R/E R/E-TYPE 5%I II I' , 5.0)\n",
    "# case 4: replace US_OFFLINES TO US\n",
    "update_values(data, \"pos\", 'US_OFFLINES' , 'US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check after update\n",
    "check_unique_values(['agent_discount', 'corporate_acc_id', 'corporate_discount',\n",
    "       'ct_fare_comm', 'discount_unit', 'fbc', 'flights', 'pcc', 'pos', 'rbd', 'rtw_fare_comm',\n",
    "       'sales_period_dis_date', 'sales_period_eff_date', 'tkt_designator',\n",
    "       'tkt_dis_date', 'trv_dis_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"CXcommissions_etl_v1.csv\")\n",
    "#data = pd.read_csv(\"CXcommissions_etl_v1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise the table: Create commissions, commissions_loc and commissions_loc_map "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split the table to main and locations, create foreign key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index to create foreign key\n",
    "data.sort_values(by=['from_location', 'to_location', 'pos'], inplace = True)\n",
    "data.reset_index(drop=True, inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = data[['from_location', 'to_location', 'pos']]\n",
    "comm_cols = ['agent_discount', 'corporate_acc_id', 'corporate_discount',\n",
    "       'ct_fare_comm', 'discount_unit', 'fbc', 'filename', 'flights',\n",
    "       'originating', 'pcc', 'pos', 'rbd', 'rtw_fare_comm',\n",
    "       'sales_period_dis_date', 'sales_period_eff_date', 'tkt_designator',\n",
    "       'tkt_dis_date', 'tourcodes', 'trv_dis_date', '_id']\n",
    "commission_data = data[comm_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create foreign key\n",
    "commission_data['loc_fk'] = commission_data.index\n",
    "locations.drop_duplicates(inplace=True)\n",
    "# create foreign key to link between the two table\n",
    "fk_lst = []\n",
    "for i in range(len(locations.index)):\n",
    "    index = locations.index[i]\n",
    "    if i < len(locations.index)-1:\n",
    "        next_index = locations.index[i+1]\n",
    "    elif i == len(locations.index)-1:\n",
    "        next_index = len(commission_data)\n",
    "    index_lst = [i]*(next_index-index)\n",
    "    print(index, next_index, next_index-index)\n",
    "    fk_lst = fk_lst + index_lst\n",
    "len(fk_lst)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations.reset_index(drop=True, inplace=True)\n",
    "locations['loc_fk'] = locations.index\n",
    "commission_data['loc_fk'] = fk_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create location mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup\n",
    "location_copy = locations.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recover\n",
    "locations = location_copy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find unique location pos matching\n",
    "location_map = locations[['from_location', 'pos']].values.tolist()+locations[['to_location', 'pos']].values.tolist()\n",
    "location_map = pd.DataFrame(location_map)\n",
    "location_map.drop_duplicates(inplace=True)\n",
    "location_map.columns = ['location', 'pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check location\n",
    "location_map['location'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### faulty data found in locations, further data cleaning for location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JFKHKG(VIA CX888/889 ONLY) case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations.loc[locations['from_location'] == 'JFKHKG(VIA CX888/889 ONLY)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update location table \n",
    "locations.loc[locations['from_location'] == 'JFKHKG(VIA CX888/889 ONLY)', 'from_location'] = 'JFK'\n",
    "locations.loc[locations['to_location'] == 'JFKHKG(VIA CX888/889 ONLY)', 'to_location'] = 'HKG'\n",
    "locations.loc[locations['loc_fk'] ==57]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update commission table\n",
    "dup = commission_data.loc[commission_data['loc_fk'] == 57] \n",
    "commission_data.loc[commission_data['loc_fk'] == 57, 'flights'] = '+CX888'\n",
    "dup.loc[:,'flights'] = '+CX889'\n",
    "commission_data =  pd.concat([commission_data, dup], ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "other cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_to_clean = {\n",
    "    'CX AND KA': 'ALL CX AND KA',\n",
    "    'CX AND KA SYSTEM': 'ALL CX AND KA',\n",
    "    'SYSTEM': 'ALL CX AND KA',\n",
    "    'FROM': None   \n",
    "}\n",
    "\n",
    "location_to_unwind = {\n",
    "    'MEXICO ANDLATIN AMERICA': ['MEXICO', 'LATIN AMERICA'],\n",
    "    'MEXICO AND LATIN AMERICA': ['MEXICO', 'LATIN AMERICA'], \n",
    "    'SHA OR HKG': ['SHA','HKG'],\n",
    "    'WUH, CAN, BJS, SHA OR HKG': ['WUH', 'CAN', 'BJS', 'SHA', 'HKG']    \n",
    "}\n",
    "     \n",
    "location_route = {\n",
    "    'BOS SIN AND BOS SHA': ('BOS', ['SIN', 'SHA']),\n",
    "    'BOS TO PVG': ('BOS', 'PVG'),\n",
    "    'CANADA TO': ('CANADA', None),\n",
    "    'CANADA TO ASIA': ('CANADA', 'ASIA'),\n",
    "    'CHI OR CID TO ASIA': (['CHI','CID'], 'ASIA'),\n",
    "    'LAX TO ASIA': ('LAX', 'ASIA'),\n",
    "    'LAX TO ASIA ONLY': ('LAX', 'ASIA'),\n",
    "    'SHA OR HKG': (['SHA', 'HKG'],['SHA', 'HKG']),\n",
    "    'TOWUH, CAN, BJS,': (None, ['WUH', 'CAN', 'BJS']),\n",
    "    'USA ORLATIN AMERICA - ASIA': ('USA', 'ASIA'),\n",
    "    'CANADA â€“ ASIA*': ('CANADA', 'ASIA'),\n",
    "    'JFK/EWR-AISA': (['EWR', 'JFK'], 'ASIA'),\n",
    "    'USA-AISA': ('USA', 'ASIA'),\n",
    "    'USA TO ASIA': ('USA', 'ASIA'),\n",
    "    'USA ASIA': ('USA', 'ASIA') \n",
    "}\n",
    "# investigate this case: 'JFKHKG(VIA CX888/889 ONLY)'\n",
    "target_cols = ['from_location', 'to_location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean step 1: change route to from and to location\n",
    "def elem_convert(elem):\n",
    "    if isinstance(elem, list):\n",
    "        elem_return = ' '.join(elem)\n",
    "        location_to_unwind[elem_return] = elem\n",
    "    else:\n",
    "        elem_return = elem\n",
    "    return elem_return\n",
    "\n",
    "# transform data based on location_route into location_to_clean and location_to_unwind\n",
    "clean_data1 = locations.copy()\n",
    "for tc in target_cols:\n",
    "    for k, v in location_route.items():\n",
    "        if len(clean_data1.loc[locations[tc]==k])>0:\n",
    "            clean_data1.loc[locations[tc]==k, target_cols[0]] = elem_convert(v[0])\n",
    "            clean_data1.loc[locations[tc]==k, target_cols[1]] = elem_convert(v[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning step 2: replace the faulty value of a location field\n",
    "def replace(k, v, tcols, locations, location_to_clean):\n",
    "    new_locations = locations.copy()\n",
    "    for tc in tcols:\n",
    "        for k, v in location_to_clean.items():\n",
    "            new_locations.loc[new_locations[tc] == k, tc] = v\n",
    "    return new_locations\n",
    "clean_loc2 = replace(k, v, target_cols, clean_data1, location_to_clean)\n",
    "clean_loc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean step 3 : unwind the field that has list\n",
    "def replace_and_unwind(k, v, tc, locations_old, target_cols):\n",
    "    locations_new = locations_old.copy()\n",
    "    # if the faulty value exists\n",
    "    if len(locations_new.loc[locations_old[tc] == k]) > 0:\n",
    "        # print(\"updating: \", tc, k, v, len(locations_new))\n",
    "        # replace the value with the first element of the list\n",
    "        locations_new.loc[locations_old[tc] == k, tc] = v[0]\n",
    "        # for the rest of the elements\n",
    "        # create duplicate rows\n",
    "        dups = []\n",
    "        for v_e in v[1:]:\n",
    "            # create duplicate rows\n",
    "            dup = locations_new.loc[locations_old[tc] == k]\n",
    "            # replace target column value in duplicate rows\n",
    "            dup[tc] = v_e\n",
    "            dups.append(dup)\n",
    "        # add in the duplicate rows  \n",
    "        for dup in dups:\n",
    "            locations_new = pd.concat([locations_new, dup], ignore_index = True)\n",
    "        # print(\"after update: \", tc, k, v, \"length of datafrane: \", len(locations_new))      \n",
    "    # print(\"after update all: \", k, v, \"length of datafrane: \", len(locations_new))\n",
    "    return locations_new\n",
    "# cleaning\n",
    "\n",
    "clean_loc3 = clean_loc2.copy()\n",
    "for tc in target_cols:\n",
    "    for k, v in location_to_unwind.items():\n",
    "        # print(\"---before update----\", len(location_clean))\n",
    "        clean_loc3= replace_and_unwind(k, v, tc, clean_loc3, target_cols)       \n",
    "        # print(\"-------------after update--------------\", len(location_clean))\n",
    "        # print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = clean_loc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate location map\n",
    "location_map = clean_loc3[['from_location', 'pos']].values.tolist()+clean_loc3[['to_location', 'pos']].values.tolist()\n",
    "location_map = pd.DataFrame(location_map)\n",
    "location_map.drop_duplicates(inplace=True)\n",
    "location_map.columns = ['location', 'pos']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find location code mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('mysql+pymysql://jialuc:test@localhost/genie')\n",
    "db_conn = engine.connect() \n",
    "# get location code information from databse\n",
    "airport_city_map = pd.read_sql('SELECT airpt_cd, ctry_nm, ctry_cd, city_nm FROM genie.ref_airpt_city;', con=db_conn)\n",
    "zone_map = pd.read_sql('SELECT zone, area_desc, cntry, cntry_cd FROM genie.ref_atpco_zone;', con=db_conn)\n",
    "iata_city_map = pd.read_sql('SELECT airpt_cd, cntry_cd, city_cd, atpco_zone FROM genie.iata_airport_city;', con=db_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### match by airport code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match airport code with location\n",
    "location_map = location_map.set_index('location').join(iata_city_map.set_index('airpt_cd'), how='left')\n",
    "# location_map = location_map.set_index('location').join(airport_city_map.set_index('airpt_cd'), how='left')\n",
    "# create airport code column, is null when is not\n",
    "location_map['airpt_cd'] = location_map.index\n",
    "mask = location_map[['cntry_cd', 'city_cd', 'atpco_zone']].sum(axis=1) == 0\n",
    "# mask = location_map[['ctry_nm', 'ctry_cd', 'city_nm']].sum(axis=1) == 0\n",
    "location_map.loc[mask, 'airpt_cd'] = ''\n",
    "location_map['location'] = location_map.index\n",
    "location_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the entries are matched by airport code\n",
    "# if the airport code and the city code are the same, use the city code \n",
    "mask1 = location_map['airpt_cd'] != ''\n",
    "mask2 = location_map['airpt_cd'] == location_map['city_cd']\n",
    "location_map.loc[mask1&mask2, 'code'] = location_map['city_cd'] \n",
    "location_map.loc[mask1&mask2, 'loc_type'] = 'C'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# otherwise use airport\n",
    "mask3 = location_map['airpt_cd'] != location_map['city_cd']\n",
    "location_map.loc[mask1&mask3, 'code'] = location_map['airpt_cd']\n",
    "location_map.loc[mask1&mask3, 'loc_type'] = 'P'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of the other useless columns\n",
    "location_map.drop('cntry_cd', inplace=True, axis=1)\n",
    "location_map.drop('airpt_cd', inplace=True, axis=1)\n",
    "location_map.drop('atpco_zone', inplace=True, axis=1)\n",
    "location_map.drop('city_cd', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find unmatch: \n",
    "mask = location_map[['code', 'loc_type']].sum(axis=1) == 0\n",
    "location_map.loc[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### maunally encode these cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_map.loc['USA', 'code'] = 'US'\n",
    "location_map.loc['USA', 'loc_type'] = 'N'\n",
    "\n",
    "location_map.loc['JKT', 'code'] = 'JKT'\n",
    "location_map.loc['JKT', 'loc_type'] = 'P'\n",
    "\n",
    "# location_map.loc['BJS', 'code'] = 'BJS'\n",
    "# location_map.loc['BJS', 'loc_type'] = 'C'\n",
    "\n",
    "# location_map.loc['CANADA', 'code'] = 'CA'\n",
    "# location_map.loc['CANADA', 'loc_type'] = 'N'\n",
    "\n",
    "# location_map.loc['MEXICO', 'code'] = 'MX'\n",
    "# location_map.loc['MEXICO', 'loc_type'] = 'N'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### solve asia case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup = location_map.loc[['ASIA']]\n",
    "dup = pd.concat([dup]*4, ignore_index=True)\n",
    "zones = ['310', '320', '330', '340']\n",
    "for index, row in dup.iterrows():\n",
    "    dup.loc[index, 'code'] = zones[index]\n",
    "    dup.loc[index, 'loc_type'] = 'Z'\n",
    "dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_map.loc['ASIA', 'code'] = '220'\n",
    "location_map.loc['ASIA', 'loc_type'] = 'Z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_map = pd.concat([location_map, dup], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### solve latin america case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup1 = location_map.loc[['LATIN AMERICA']]\n",
    "dup1 = pd.concat([dup1]*4, ignore_index=True)\n",
    "zones = ['120', '140', '160', '170']\n",
    "for index, row in dup.iterrows():\n",
    "    dup1.loc[index, 'code'] = zones[index]\n",
    "    dup1.loc[index, 'loc_type'] = 'Z'\n",
    "dup1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_map.loc['LATIN AMERICA', 'code'] = 'MX'\n",
    "location_map.loc['LATIN AMERICA', 'loc_type'] = 'N'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_map['location'] = location_map.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_map = pd.concat([location_map, dup1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check unmatch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unmatch: \n",
    "mask = location_map[['code', 'loc_type']].sum(axis=1) == 0\n",
    "location_map.loc[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### previous code, ignore and go to importing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zone_map.drop('cntry_cd', axis=1, inplace=True)\n",
    "zone_map.columns = ['zone', 'zone_desc', 'zone_country']\n",
    "# turn country name to upper case\n",
    "zone_map['zone_country']=zone_map['zone_country'].str.upper()\n",
    "# join by country name \n",
    "location_map = location_map.join(zone_map.set_index('zone_country'), how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1 = location_map[['zone', 'zone_desc']].sum(axis=1) != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location that has been matched by zone\n",
    "location_map.loc[mask1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete some of the wrong matching\n",
    "location_map = location_map.loc[location_map['zone']!='000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_map.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find unmatch:\n",
    "mask1 = location_map[['cntry_cd', 'city_cd', 'atpco_zone', 'airpt_cd']].sum(axis=1) == 0\n",
    "mask2 = location_map[['zone', 'zone_desc']].sum(axis=1) == 0\n",
    "location_map.loc[mask2&mask1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_map.loc['ASIA', 'zone'] = '320'\n",
    "location_map.loc['BJS', 'city_cd'] = 'BJS'\n",
    "location_map.loc['BJS', 'cntry_cd'] = 'CN'\n",
    "location_map.loc['BJS', 'atpco_zone'] = '320'\n",
    "location_map.loc['LATIN AMERICA', 'atpco_zone'] = '320'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_map.loc[mask2&mask1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete empty entry of lcoations\n",
    "location_map = location_map[location_map.index!='']\n",
    "location_map = location_map[location_map.index.notna()]\n",
    "location_map['location'] = location_map.index\n",
    "location_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### solve case: JFKHKG(VIA CX888/889 ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations.loc[locations['from_location'] == 'JFKHKG(VIA CX888/889 ONLY)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations.loc[locations['from_location'] == 'JFKHKG(VIA CX888/889 ONLY)', ['from_location', 'to_location']] = ['JFK', 'HKG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations.iloc[58]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'JFK' in location_map.index and 'HKG' in location_map.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commission_data.loc[commission_data['loc_fk'] == 58, ['flights']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the commission only applys via (VIA CX888/889 ONLY)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_map_bu = location_map.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge zone with atpco_zone\n",
    "mask1 = location_map['zone'].isna()\n",
    "location_map.loc[mask1, 'zone'] = ''\n",
    "mask2 = location_map['atpco_zone'].isna()\n",
    "location_map.loc[mask2, 'atpco_zone'] = ''\n",
    "location_map['new_zone'] = location_map['zone'] + location_map['atpco_zone']\n",
    "location_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_map['zone'] = location_map['new_zone']\n",
    "location_map.drop('atpco_zone', inplace=True, axis=1)\n",
    "location_map.drop('new_zone', inplace=True, axis=1)\n",
    "location_map.drop('zone_desc', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### unsolve case: ALL CX, JFKHKG(VIA CX888/889 ONLY), LATIN AMERICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_map.to_csv('commissions_loc_map.csv')\n",
    "commission_data.to_csv('commissions.csv')\n",
    "locations.to_csv('commissions_loc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transform the location map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_by_city = location_map['location'] = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### commissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for reading csv file directly \n",
    "# path of the csv file that stores the source files directory\n",
    "\n",
    "# dir_path = \"CXcommissions_201807051523.csv\"\n",
    "# dat=pd.read_csv(dir_path)\n",
    "# dat.drop(['Unnamed: 0'], axis=1, inplace = True)\n",
    "# dat.columns = [col.lower() for col in dat.columns]\n",
    "# dat['commission_id'] = np.nan\n",
    "# dat_cols = list(dat.columns)\n",
    "# dat_cols.insert(0, dat_cols.pop())\n",
    "# dat = dat[dat_cols]\n",
    "# dat=pd.read_csv(dir_path)\n",
    "# dat.drop(['Unnamed: 0'], axis=1, inplace = True)\n",
    "# dat.columns = [col.lower() for col in dat.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the import data\n",
    "#commission_data\n",
    "#location_map\n",
    "#locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commission_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commission_data['tkt_designator'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transform data types to match with sqlalchemy: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify columns in each data type for commission_data\n",
    "int_type = []\n",
    "float_type = ['agent_discount', 'corporate_discount']\n",
    "date_type = ['sales_period_dis_date', 'sales_period_eff_date', 'tkt_dis_date']\n",
    "str_type = ['loc_fk', 'pcc', 'ct_fare_comm', 'rtw_fare_comm', 'country', 'discount_unit', 'fbc', 'filename', 'flights', \n",
    "            'originating', 'tkt_designator','corporate_acc_id', 'pos', 'rbd', 'tourcodes', '_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(str_type)+len(date_type)+len(float_type) == len(commission_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# data type mapper+transformation function\n",
    "def type_mapper(col, index, row):\n",
    "    no_error = True\n",
    "    dat = row[col]\n",
    "    try:\n",
    "        if col in int_type:\n",
    "            row[col] = float(dat)       \n",
    "        if col in float_type:\n",
    "            row[col] = float(dat)\n",
    "        if col in str_type:\n",
    "            row[col] = str(dat)\n",
    "        if col in date_type:\n",
    "            date = datetime.strptime(dat,'%Y-%m-%d')\n",
    "            row[col] = date.date()\n",
    "    except TypeError:\n",
    "        no_error = False\n",
    "        print(col, row[col])\n",
    "    except ValueError: \n",
    "        no_error = False\n",
    "        print(col, row[col])\n",
    "    return no_error, row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "# convert data for importing\n",
    "commissions = []\n",
    "for index, row in commission_data.iterrows():\n",
    "    no_error = True\n",
    "    row = dict(row)\n",
    "    not_nan_cols = []\n",
    "    # if the field is nan\n",
    "    for col in commission_data.columns:\n",
    "        if str(row[col]) == 'nan' or row[col] == '' or row[col] == 'None':\n",
    "            row[col] = \"\"\n",
    "        # if is not nan\n",
    "        else:\n",
    "            not_nan_cols.append(col)\n",
    "    # for all columns that are not nan field\n",
    "    for col in not_nan_cols:\n",
    "        this_no_error, row = type_mapper(col, index, row)\n",
    "        no_error = no_error and this_no_error\n",
    "    if no_error:\n",
    "        commissions.append(row)\n",
    "    else:\n",
    "        print(\"Skip row due to conversion error: \", index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add run id\n",
    "import datetime\n",
    "time= datetime.now()\n",
    "\n",
    "for com in commissions:\n",
    "    com['run_id'] = time.strftime('%Y%m%H%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('mysql+pymysql://jialuc:12345678@test@localhost/genie/cx_dw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from  sqlalchemy.schema import Column\n",
    "from sqlalchemy import *\n",
    "\n",
    "# create table in database\n",
    "Base = declarative_base()\n",
    "class Commissions(Base):\n",
    "    __tablename__=\"commissions\"\n",
    "    agent_discount = Column(Numeric(6))\n",
    "    corporate_discount = Column(Numeric(6))\n",
    "    # country = Column(String(4))\n",
    "    ct_fare_comm = Column(String(10))\n",
    "    discount_unit = Column(String(4)) \n",
    "    fbc = Column(String(7))\n",
    "    filename = Column(String(200))\n",
    "    flights = Column(String(100),nullable=True, primary_key=True) \n",
    "    # from_location = Column(String(100))\n",
    "    originating = Column(String(50)) \n",
    "    pos = Column(String(50)) \n",
    "    rbd = Column(String(4)) \n",
    "    rtw_fare_comm = Column(String(10))\n",
    "    sales_period_dis_date = Column(Date) \n",
    "    sales_period_eff_date = Column(Date) \n",
    "    tkt_dis_date = Column(Date)\n",
    "    tourcodes = Column(String(100)) \n",
    "    # to_location = Column(String(100)) \n",
    "    trv_dis_date = Column(Date)\n",
    "    pcc = Column(String(10),nullable=True) \n",
    "    tkt_designator = Column(String(50),nullable=True)  \n",
    "    corporate_acc_id = Column(String(20),nullable=True)\n",
    "    _id = Column(String(40), primary_key=True)\n",
    "    run_id = Column(String(12), primary_key=True)\n",
    "    loc_fk = Column(Integer,)\n",
    "# create commissions table if not exists\n",
    "Commissions.__table__.create(bind=engine, checkfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.orm.session import sessionmaker\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session() \n",
    "for com in commissions:\n",
    "    row = Commissions(**com)\n",
    "    session.add(row)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import locations into MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from  sqlalchemy.schema import Column\n",
    "from sqlalchemy import *\n",
    "\n",
    "# create table in database\n",
    "Base = declarative_base()\n",
    "class Locations(Base):\n",
    "    __tablename__=\"commissions_loc\"\n",
    "    from_location = Column(String(100), nullable=True, primary_key=True)\n",
    "    pos = Column(String(50), nullable=True, primary_key=True) \n",
    "    to_location = Column(String(100), nullable=True, primary_key=True) \n",
    "    loc_fk = Column(Integer, primary_key=True)\n",
    "# create commissions table if not exists\n",
    "Locations.__table__.create(bind=engine, checkfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_type = ['loc_fk']\n",
    "str_type = ['from_location', 'to_location', 'pos']\n",
    "date_type = []\n",
    "float_type = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the type\n",
    "from datetime import datetime, timedelta\n",
    "commissions_loc = []\n",
    "for index, row in locations.iterrows():\n",
    "    no_error = True\n",
    "    row = dict(row)\n",
    "    not_nan_cols = []\n",
    "    # if the field is nan\n",
    "    for col in locations.columns:\n",
    "        if str(row[col]) == 'nan' or row[col] == '' or row[col] == 'None':\n",
    "            row[col] = None\n",
    "        # if is not nan\n",
    "        else:\n",
    "            not_nan_cols.append(col)\n",
    "    # for all columns that are not nan field\n",
    "    for col in not_nan_cols:\n",
    "        this_no_error, row = type_mapper(col, index, row)\n",
    "        no_error = no_error and this_no_error\n",
    "    if no_error:\n",
    "        commissions_loc.append(row)\n",
    "    else:\n",
    "        print(\"Skip row due to conversion error: \", index, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(commissions_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "from sqlalchemy.orm.session import sessionmaker\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session() \n",
    "for com_loc in commissions_loc:\n",
    "    row = Locations(**com_loc)\n",
    "    session.add(row)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import location_map into MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify columns in each data type for commission_data\n",
    "int_type = []\n",
    "float_type = []\n",
    "str_type = ['pos', 'loc_type', 'code', 'location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base = declarative_base()\n",
    "class Maps(Base):\n",
    "    __tablename__=\"commissions_loc_map\"\n",
    "    pos = Column(String(10), nullable=True, primary_key=True)\n",
    "    location = Column(String(50), nullable=True, primary_key=True)\n",
    "    code = Column(String(3), nullable=True, primary_key=True)\n",
    "    loc_type = Column(String(1), nullable=True, primary_key=True)\n",
    "# create commissions table if not exists\n",
    "Maps.__table__.create(bind=engine, checkfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "commissions_map = []\n",
    "for index, row in location_map.iterrows():\n",
    "    no_error = True\n",
    "    row = dict(row)\n",
    "    not_nan_cols = []\n",
    "    # if the field is nan\n",
    "    for col in location_map.columns:\n",
    "        if str(row[col]) == 'nan' or row[col] == '' or row[col] == 'None' or row[col] == None:\n",
    "            # print(index, col)\n",
    "            row[col] = None\n",
    "            # print(row[col])\n",
    "        # if is not nan\n",
    "        else:\n",
    "            not_nan_cols.append(col)\n",
    "    # for all columns that are not nan field\n",
    "    for col in not_nan_cols:\n",
    "        this_no_error, row = type_mapper(col, index, row)\n",
    "        no_error = no_error and this_no_error\n",
    "    if no_error:\n",
    "        commissions_map.append(row)\n",
    "    else:\n",
    "        print(\"Skip row due to conversion error: \", index, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.orm.session import sessionmaker\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session() \n",
    "for com_loc in commissi\n",
    "ons_map:\n",
    "    row = Maps(**com_loc)\n",
    "    session.add(row)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import with csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"CXcommissions_etl.csv\"\n",
    "schema = \"zz_jc\"\n",
    "table_name = \"commissions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_sql = \"\"\"LOAD DATA LOCAL INFILE 'CXcommissions_etl.csv' INTO TABLE zz_jc.commissions_etl FIELDS TERMINATED BY ',' ENCLOSED BY '\"' IGNORE 1 LINES;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_store_commissions(hostname, username, password, database):\n",
    "\n",
    "    '''\n",
    "    This function load a csv file to MySQL table according to\n",
    "    the load_sql statement.\n",
    "    '''\n",
    "    try:\n",
    "        con = pymysql.connect(host=hostname,\n",
    "                                user=username,\n",
    "                                password=password,\n",
    "                                local_infile=1)\n",
    "        \n",
    "        print('Connected to DB: {}'.format(hostname))\n",
    "        # Create cursor and execute Load SQL\n",
    "        cursor = con.cursor()\n",
    "        truncate_Stmt = \"truncate zz_jc.commissions_etl\"\n",
    "        cursor.execute(truncate_Stmt)\n",
    "        cursor.execute(load_sql)\n",
    "        print('Succuessfully loaded the table from csv.')\n",
    "        con.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print('Error: {}'.format(str(e)))\n",
    "        sys.exit(1)\n",
    "\n",
    "connect_store_commissions(hostname, username, password, database)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
